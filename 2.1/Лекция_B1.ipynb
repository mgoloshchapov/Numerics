{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Лекция B1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "8ctTsaXdQm5E",
        "a2pKNXDYOOwt",
        "nOC9RhkOOO9n",
        "GF6G6ZK6OPGO",
        "5Tu0ufz1OPgX",
        "10whefwsOPnu",
        "kVTBJLsrOWvu"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AqydNtCLYIA"
      },
      "source": [
        "#Лекция 1\n",
        "\n",
        "*  Введение\n",
        "*  СЛАУ и задачи, в которых они возникают\n",
        "*  Методы решения СЛАУ из линейной алгебры\n",
        "*  Погрешность вычислений\n",
        "*  Норма вектора\n",
        "*  Норма матрицы\n",
        "*  Норма Фробениуса и неоператорные нормы матриц "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ctTsaXdQm5E"
      },
      "source": [
        "##Введение \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxDP7JTqIZIW"
      },
      "source": [
        "Начнем блок, который называется \"Численные методы линейной алгебры\". В данном блоке будут рассмотрены основные численные методы решения основных задач линейной алгебры. Некоторые примеры таких задач:\n",
        "*  Решение СЛАУ\n",
        "*  Нахождение обратной матрицы \n",
        "*  Нахождение собственных значений и векторов матрицы"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2pKNXDYOOwt"
      },
      "source": [
        "##СЛАУ и задачи, в которых они возникают\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7zlq27NIInv"
      },
      "source": [
        "Системы линейных алгебраических уравнений возникают в широком пласте задач. Например, при решении дифференциальных уравнений в частных производных, при нахождении пересечения гиперплоскостей и\\или прямых.  \n",
        "\n",
        "Записываются СЛАУ в виде :\n",
        "\n",
        "$\\begin{cases}\n",
        "\\alpha_{1, 1}x_1 + \\alpha_{1, 2}x_2 + ... + \\alpha_{1, n}x_n = b_1 \\\\\n",
        "\\alpha_{2, 1}x_1 + \\alpha_{2, 2}x_2 + ... + \\alpha_{2, n}x_n = b_2 \\\\\n",
        "... \\\\\n",
        "\\alpha_{m, 1}x_1 + \\alpha_{m, 2}x_2 + ... + \\alpha_{m, n}x_n = b_m \\\\\n",
        "\\end{cases}$\n",
        "\n",
        "Перепишем в матричном виде :  $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOC9RhkOOO9n"
      },
      "source": [
        "##Методы решения СЛАУ из линейной алгебры\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wM1idm2sITZu"
      },
      "source": [
        "Попробуем решить систему одним из уже известных методов:\n",
        "\n",
        "Методы, известные из курса линейной алгебры:\n",
        "*  Метод Гаусса - сведение матрицы к верхнетреугольной матрице. Сложность $O(n^3)$\n",
        "*  Правило Крамера - $x_i = \\frac{\\Delta_i}{\\Delta}$, где $\\Delta$ - определитель. Сложность вычисления определителя $n!$, а посчитать необходимо $n+1$ определитель.\n",
        "*  Нахождение обратной матрицы - $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$, откуда $\\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b}$. Сложность $O(n^3)$ (при нахождении методом Гаусса) или $O(n^2)O_{det}$ (при использовании алгебраических дополнений). Здесь  $O_{det}$ - сложность вычисления определителя матрицы.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF6G6ZK6OPGO"
      },
      "source": [
        "##Погрешность вычислений\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpNCjJbiIdcG"
      },
      "source": [
        "\n",
        "Зачастую данные для задачи получают в ходе эксперимента. В таком случае данные никогда не будут точными, в них всегда будет некоторая случайная ошибка.\n",
        "\n",
        "$\\mathbf{A}\\mathbf{x} = \\mathbf{b} +\\mathbf{\\delta b}$\n",
        "\n",
        "Пусть $x$ - точное значение величины, а $x^*$ - измеренное.\n",
        "\n",
        "Введем несколько определений : \n",
        "*  Абсолютная погрешность величины $x^*$  \n",
        "$\\Delta (x^*)\\geq|x^* - x|$\n",
        "*  Относительная погрешность величины $x^*$      \n",
        " $\\delta(x^*)\\geq\\frac{|x^* - x|}{x^*}$\n",
        "\n",
        "Рассмотрим различные возможные погрешности:\n",
        "*  Возмущение входных данных \n",
        "*  Погрешность метода. Рассмотреть на примере ряда Тейлора.\n",
        "*  Ошибки вычислений\n",
        "\n",
        "Приближенное представление чисел в компьютере один из основных источников вычислительных погрешностей. Число $x$, не представимое в компьютере заменяется близким числом $x^*$, представимым в компьютере точно. Вычислим эту погрешность. Рассмотрим простейшее округление – отбрасывание всех разрядов числа, выходящих за пределы разрядной сетки. Система счисления – двоичная. Надо записать бесконечную двоичную дробь :\n",
        "$x = \\pm2^p(0,\\alpha_1\\alpha_2\\alpha_3...)$, где $\\alpha_k$ либо $1$ либо $0$.\n",
        "\n",
        "Это число можно представить в виде суммы $x = \\pm2^p(\\sum\\limits_{i = 1}^{\\infty}\\frac{\\alpha_i}{2^i})$\n",
        "\n",
        "Записываются только $t$ двоичных разрядов, тогда \n",
        "\n",
        "$|x - x^*| \\leq 2^p\\frac{1}{2^{t + 1}}(1 + \\frac{1}{2} + ...) = 2^{p - t}$\n",
        "\n",
        "И относительная погрешность:\n",
        "\n",
        "$\\delta(x^*) \\leq 2^{1 - t}$.\n",
        "\n",
        "На практике используются более продвинутые методы, для которых в среднем верно:\n",
        "\n",
        "$\\delta(x^*) \\leq 2^{-t}$\n",
        "\n",
        "Эти методы основаны на том, чтобы округлять не вниз всегда, а случайным образом. Например, последнюю цифру записывать как операцию XOR над всеми остальными двоичными цифрами (то есть 1, если цифр 1 нечётное количество, 0 в противоположном случае).\n",
        "\n",
        "В итоге приближенное представление числа можно записать в виде $x^* = x(1 + \\varepsilon)$, где $\\varepsilon$ - машинный эпсилон.\n",
        "\n",
        "Погрешности арифметических операций: \n",
        "- при сложении и вычитании чисел необходимо складывать их абсолютные погрешности;\n",
        "- при сложении большого количества чисел, погрешности которых имеют независимый и случайный характер, целесообразнее пользоваться статистической оценкой относительной погрешности суммы: $\\quad S=x_{1}+x_{2}+\\ldots+x_{n} ; \\Rightarrow \\Delta_{S}=\\sqrt{\\Delta_{1}^{2}+\\Delta_{2}^{2}+\\ldots \\Delta_{n}^{2}}$\n",
        "- при делении и умножении чисел требуется сложить относительные погрешности;\n",
        "- при возведении в степень относительную погрешность умножают на показатель степени.\n",
        "- при перемножении большого количества чисел, погрешности которых имеют независимый и случайный характер, целесообразнее пользоваться статистической оценкой относительной погрешности произведения: $\\Pi=x_{1} x_{2} \\ldots x_{n} ; \\Rightarrow \\delta_{\\Pi}=\\sqrt{\\delta_{1}^{2}+\\delta_{2}^{2} \\ldots \\delta_{n}^{2}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Tu0ufz1OPgX"
      },
      "source": [
        "##Норма вектора\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zM5QYVgDIkOV"
      },
      "source": [
        "Обобщим длину вектора (расстояние между двумя точками). Такое обобщение называется нормой.\n",
        "\n",
        "Норма - функционал $\\mu(\\mathbf{x})$ заданный на векторном пространстве $\\mathbb{R}^n$, для которого выполняются утверждения:\n",
        "*  $\\mu(\\mathbf{x}) = 0 \\Leftrightarrow \\mathbf{x} = \\mathbf{0}$\n",
        "*  $\\mu(\\alpha \\mathbf{x}) = |\\alpha|\\mu(\\mathbf{x})$\n",
        "*  $\\mu(\\mathbf{x} + \\mathbf{x}) \\leq \\mu(\\mathbf{x}) + \\mu(\\mathbf{x})$\n",
        "\n",
        "Норма обозначается $||\\mathbf{x}|| = \\mu(\\mathbf{x})$\n",
        "\n",
        "Рассмотрим семейство норм $||\\mathbf{x}||_p = (\\sum\\limits_{i = 1}^{n}|x_i|^n)^{\\frac{1}{n}}$\n",
        "\n",
        "*  $||\\mathbf{x}||_1 =\\sum\\limits_{i = 1}^{n} |x_i| $ - L1 норма, манхэттенское расстояние, октаэдрическая .\n",
        "*  $||\\mathbf{x}||_2 = (\\sum\\limits_{i = 1}^{n}x_i^2)^{\\frac{1}{2}}$ - L2 норма, евклидова норма.\n",
        "*  $||\\mathbf{x}||_{\\infty} = \\max\\limits_{i}|x_i|$ - кубическая норма.\n",
        "\n",
        "Теорема о константе эквивалентности :\n",
        "Рассмотрим векторное пространство $\\mathbb{R}^n$. Тогда :\n",
        "\n",
        "$\\forall p,q \\exists C_1, C_2>0  \\forall x \\in \\mathbb{R}^n : C_1||\\mathbf{x}||_p\\leq||\\mathbf{x}||_q\\leq C_2||\\mathbf{x}||_p$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10whefwsOPnu"
      },
      "source": [
        "##Норма матрицы\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUNOJCjUInjV"
      },
      "source": [
        "Можно ввести нормы матриц аналогично векторным нормам, но такие нормы практически не используются (за исключением неоператорной нормы Фробениуса).\n",
        "\n",
        "Норма матрицы должна удовлетворять стандартным аксиомам нормы:\n",
        "- $\\|\\mathbf{A}\\|=0 \\quad \\Leftrightarrow \\quad \\mathbf{A}=\\mathbf{0}$\n",
        "- $\\|\\alpha \\mathbf{A}\\|=|\\alpha|\\|\\mathbf{A}\\|, \\quad \\forall \\alpha \\in \\mathbb{R}$\n",
        "- $\\|\\mathbf{A}+\\mathbf{B}\\| \\leqslant\\|\\mathbf{A}\\|+\\|\\mathbf{B}\\| .$\n",
        "\n",
        "\n",
        "Матричная норма $\\|\\mathbf{A}\\|$ называется согласованной с векторной нормой $\\|\\mathrm{x}\\|$, если выполняется соотношение\n",
        "\n",
        "$$\n",
        "\\|\\mathbf{y}\\| \\leqslant\\|\\mathbf{A}\\| \\cdot\\|\\mathbf{x}\\|, \\quad \\text { где } \\mathbf{y}=\\mathbf{A} \\mathbf{x}\n",
        "$$\n",
        "\n",
        "Матричная норма называется подчинённой векторной норме, если \n",
        "\n",
        "$||\\mathbf{A}\\|| = \\sup\\limits_{|\\mathbf{x}|\\neq 0}\\frac{||\\mathbf{A} \\mathbf{x}||}{|\\mathbf{x}|}$ или $||\\mathbf{A}\\|| = \\sup\\limits_{|\\mathbf{x}| = 1}\\||\\mathbf{A} \\mathbf{x}||$\n",
        "\n",
        "\n",
        "• Норма матрицы\n",
        "\n",
        "$\\|\\mathbf{A}\\|_{1}=\\max\\limits_{j}(\\sum\\limits_{i = 1}^{n}|a_{ij}|)$, подчинённая векторной норме $\\|\\mathbf{x}\\|_{1}=\\sum\\limits_{i = 1}^{n}|x_{i}|$.\n",
        "\n",
        "\n",
        "• Норма матрицы \n",
        "\n",
        "$\\|\\mathbf{A}\\|_{\\infty}=\\max\\limits_{i}(\\sum\\limits_{j = 1}^{n}|a_{ij}|)$, подчинённая векторной норме $\\|\\mathbf{x}\\|_{\\infty}=\\max\\limits_{i}(|x_{i}|)$.\n",
        "\n",
        "• Спектральная норма \n",
        "\n",
        "$\\|\\mathbf{A}\\|_{2}=\\sup _{\\|\\mathbf{x}\\|_{2}=1}\\|\\mathbf{A} \\mathbf{x}\\|_{2}=\\sup _{(\\mathbf{x}, \\mathbf{x})=1} \\sqrt{(\\mathbf{A} \\mathbf{x}, \\mathbf{A} \\mathbf{x})}=\\sqrt{\\lambda_{\\max }\\left(\\mathbf{A}^{*} \\mathbf{A}\\right)}$, подчиненная векторной норме $\\|\\mathbf{x}\\|_{2}=\\sqrt{\\sum\\limits_{i = 1}^{n}|x_{i}|^{2}}$\n",
        "\n",
        "\n",
        "\n",
        "Матричная норма $\\|\\mathbf{A}\\|$ называется субмультипликативной, если\n",
        "\n",
        "$$\n",
        "\\|\\mathbf{A} \\cdot \\mathbf{B}\\| \\leqslant\\|\\mathbf{A}\\| \\cdot\\|\\mathbf{B}\\|\n",
        "$$\n",
        "\n",
        "Если норма $\\|\\mathbf{A}\\|$ подчинена какой-то векторной норме $\\|\\mathbf{x}\\|$, то она субмультипликативна:\n",
        "\n",
        "$$\n",
        "\\|\\mathbf{A} \\cdot \\mathbf{B}\\|=\\sup _{\\mathbf{x} \\neq 0} \\frac{\\|\\mathbf{A B} \\mathbf{x}\\|}{\\|\\mathbf{x}\\|}=\\sup _{\\mathbf{B} \\mathbf{x} \\neq \\mathbf{0}} \\frac{\\|\\mathbf{A B} \\mathbf{x}\\|}{\\|\\mathbf{x}\\|} \\leqslant \\sup _{\\mathbf{B} \\mathbf{x} \\neq \\mathbf{0}} \\frac{\\|\\mathbf{A B} \\mathbf{x}\\|}{\\|\\mathbf{B} \\mathbf{x}\\|} \\sup _{\\mathbf{B} \\mathbf{x} \\neq \\mathbf{0}} \\frac{\\|\\mathbf{B} \\mathbf{x}\\|}{\\|\\mathbf{x}\\|} \\leqslant\n",
        "$$\n",
        "Последние два супремума взяты по части множества ненулевых векторов. От увеличения множества они не уменьшатся:\n",
        "\n",
        "$$\n",
        "\\leqslant \\sup _{\\mathbf{y} \\neq \\mathbf{0}} \\frac{\\|\\mathbf{A} \\mathbf{y}\\|}{\\|\\mathbf{y}\\|} \\sup _{\\mathbf{z} \\neq \\mathbf{0}} \\frac{\\|\\mathbf{B} \\mathbf{z}\\|}{\\|\\mathbf{z}\\|}=\\|\\mathbf{A}\\| \\cdot\\|\\mathbf{B}\\|\n",
        "$$ \n",
        "\n",
        "Если норма $\\|\\mathbf{A}\\|$ подчинена какой-то векторной норме $\\|\\mathbf{x}\\|$, то она с ней согласована:\n",
        "$$\n",
        "\\|\\mathbf{A}\\|=\\sup _{\\mathbf{x} \\neq \\mathbf{0}} \\frac{\\|\\mathbf{A} \\mathbf{x}\\|}{\\|\\mathbf{x}\\|} \\Rightarrow\\|\\mathbf{A} \\mathbf{x}\\| \\leqslant\\|\\mathbf{A}\\| \\cdot\\|\\mathbf{x}\\|\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVTBJLsrOWvu"
      },
      "source": [
        "##Норма Фробениуса и неоператорные нормы матриц."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvTG1MATOh0U"
      },
      "source": [
        "Неоператорные нормы - это нормы, которые не являются подчинёнными какой-либо векторной норме. В свою очередь, подчинённые нормы также называют операторными нормами матриц.\n",
        "\n",
        "Норма Фробениуса, или евклидова норма (для евклидового пространства) представляет собой частный случай $p$-нормы для $p=2$ :\n",
        "\n",
        "$$\n",
        "\\|\\mathbf{A}\\|_{F}=\\sqrt{\\sum_{i=1}^{m} \\sum_{j=1}^{n}\\left|a_{i j}\\right|^{2}}\n",
        "$$\n",
        "\n",
        "Норма Фробениуса легко вычисляется (по сравнению, например, со спектральной нормой). Обладает следующими свойствами:\n",
        "- Согласованность $:\\|\\mathbf{A}\\mathbf{x}\\|_{2} \\leq\\|\\mathbf{A}\\|_{F}\\|\\mathbf{x}\\|_{2}$, так как в силу неравенства Коши-Буняковского\n",
        "\n",
        "$$\n",
        "\\|\\mathbf{A}\\mathbf{x}\\|_{2}^{2}=\\sum_{i=1}^{m}\\left|\\sum_{j=1}^{n} a_{i j} x_{j}\\right|^{2} \\leq \\sum_{i=1}^{m}\\left(\\sum_{j=1}^{n}\\left|a_{i j}\\right|^{2} \\sum_{j=1}^{n}\\left|x_{j}\\right|^{2}\\right)=\\sum_{j=1}^{n}\\left|x_{j}\\right|^{2}\n",
        "\\|\\mathbf{A}\\|_{F}^{2}=\\|\\mathbf{A}\\|_{F}^{2}\\|\\mathbf{x}\\|_{2}^{2}\n",
        "$$\n",
        "\n",
        "- Субмультипликативность: $\\|\\mathbf{A}\\mathbf{B}\\|_{F} \\leq\\|\\mathbf{A}\\|_{F}\\|\\mathbf{B}\\|_{F}$, так как\n",
        "\n",
        "$$\n",
        "\\|\\mathbf{A}\\mathbf{B}\\|_{F}^{2}=\\sum_{i, j}\\left|\\sum_{k} a_{i k} b_{k j}\\right|^{2} \\leq \\sum_{i, j}\\left(\\sum_{k}\\left|a_{i k}\\right|\\left|b_{k j}\\right|\\right)^{2} \\leq \\sum_{i, j}\\left(\\sum_{k}\\left|a_{i k}\\right|^{2} \\sum_{k}\\left|b_{k j}\\right|^{2}\\right)=\\sum_{i, k}\\left|a_{i k}\\right|^{2} \\sum_{k, j}\\left|b_{k j}\\right|^{2}=\\|\\mathbf{A}\\|_{F}^{2}\\|\\mathbf{B}\\|_{F}^{2}\n",
        "$$\n",
        "\n",
        "- $\\|\\mathbf{A}\\|_{F}^{2}=\\operatorname{tr} \\mathbf{A}^{*} \\mathbf{A}=\\operatorname{tr} \\mathbf{A} \\mathbf{A}^{*}$, где $\\operatorname{tr} \\mathbf{A}-$ след матрицы $\\mathbf{A}, \\mathbf{A}^{*}$ - эрмитово-сопряжённая матрица.\n",
        "- $\\|\\mathbf{A}\\|_{F}^{2}=\\rho_{1}^{2}+\\rho_{2}^{2}+\\cdots+\\rho_{n}^{2}$, где $\\rho_{1}, \\rho_{2}, \\ldots, \\rho_{n}-$ сингулярные числа матрицы $\\mathbf{A} .$\n",
        "- $\\|\\mathbf{A}\\|_{F} \\geq\\|\\mathbf{A}\\|_{2}$, где $\\|\\cdot\\|_{2}-$ спектральная норма.\n",
        "- $\\|\\mathbf{A}\\|_{F}$ не изменяется при умножении матрицы $\\mathbf{A}$ слева или справа на ортогональные (унитарные) матрицы.\n",
        "\n",
        "Норма Фробениуса часто используется, потому что невязка многих методов минимизируется по ней (например, метода главных компонент). Норму Фробениуса также часто называют сферической или евклидовой, потому что она имеет простой геометрический смысл - длина вектора, который является суммой векторов - строк или столбцов матрицы. Отсюда также понятно, почему умножение матрицы на ортогональную не меняет её нормы Фробениуса. Соотношения эквивалентности её со спектральной нормой:\n",
        "\n",
        "\\begin{aligned}\n",
        "&\\|\\mathbf{A}\\|_{2} \\leq\\|\\mathbf{A}\\|_{F} \\leq \\sqrt{n}\\|\\mathbf{A}\\|_{2} \\\\\n",
        "&\\frac{1}{\\sqrt{n}}\\|\\mathbf{A}\\|_{F} \\leq\\|\\mathbf{A}\\|_{2} \\leq\\|\\mathbf{A}\\|_{F}\n",
        "\\end{aligned}"
      ]
    }
  ]
}